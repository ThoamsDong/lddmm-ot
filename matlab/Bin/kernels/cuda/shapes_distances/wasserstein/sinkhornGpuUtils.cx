// Author : B. Charlier (2017)


// Define constant to regularize the log...
#if UseCudaOnDoubles
#define EPSILON	2.220446049250313e-16
#else
#define EPSILON	1.1920929e-07
#endif

//---------------------------------------------------------------------------------------------------//
//                                A Single Sinkhorn Step                                             //
//---------------------------------------------------------------------------------------------------//

template < typename TYPE, int DIMPOINT, int DIMVECT >
__global__ void SinkhornGpuGradConvOnDevice(TYPE epsilon, TYPE lambda, TYPE weightGeom, TYPE weightGrass,
        TYPE *alpha, TYPE *x, TYPE *y, TYPE *beta, TYPE *delta, TYPE *gamma,
        int nx, int ny)
{
	int i = blockIdx.x * blockDim.x + threadIdx.x;

	// the following line does not work with nvcc 3.0 (it is a bug; it works with anterior and posterior versions)
	// extern __shared__ TYPE SharedData[];  // shared data will contain x and alpha data for the block
	// here is the bug fix (see http://forums.nvidia.com/index.php?showtopic=166905)
	extern __shared__ char SharedData_char[];
	TYPE* const SharedData = reinterpret_cast<TYPE*>(SharedData_char);
	// end of bug fix

	int const inc = DIMPOINT + DIMVECT;

	TYPE xi[DIMPOINT], alphai[DIMVECT], deltai[DIMVECT], gammai[DIMVECT], normxii = 0.0f;
	if(i<nx)  // we will compute gammai only if i is in the range
	{
		// load xi, alphai, betai from device global memory
		for(int k=0; k<DIMPOINT; k++)
			xi[k] = x[i*DIMPOINT+k];
		for(int k=0; k<DIMVECT; k++)
			alphai[k] = alpha[i*DIMVECT+k];
		for(int k=0; k<DIMVECT; k++)
			deltai[k] = delta[i*DIMVECT+k];
		for(int k=0; k<DIMVECT; k++)
			gammai[k] = 0.0f;
		
		for (int k=0; k< DIMPOINT; k++)
			normxii +=  xi[k+DIMPOINT/2] * xi[k+DIMPOINT/2];
	}

	for(int jstart = 0, tile = 0; jstart < ny; jstart += blockDim.x, tile++)
	{
		int j = tile * blockDim.x + threadIdx.x;
		if(j<ny) // we load xj, alphaj and betaj from device global memory only if j<ny
		{
			for(int k=0; k<DIMPOINT; k++)
				SharedData[threadIdx.x*inc+k] = y[j*DIMPOINT+k];
			for(int k=0; k<DIMVECT; k++)
				SharedData[threadIdx.x*inc+DIMPOINT+k] = beta[j*DIMVECT+k];
		}
		__syncthreads();

		if(i<nx) // we compute gammai only if i is in the range
		{

			TYPE *yj = SharedData, *betaj = SharedData + DIMPOINT;
			for(int jrel = 0; jrel < blockDim.x && jrel<ny-jstart; jrel++, yj+=inc, betaj+=inc)
			{
				TYPE r2 = 0.0f,  prsxiixj =0.0f, normxij = 0.0f;
				for(int k=0; k<DIMPOINT/2; k++)
				{
					TYPE ximxj =  xi[k]-yj[k];
					r2 +=  ximxj * ximxj;

					normxij +=  yj[k+DIMPOINT/2] * yj[k+DIMPOINT/2];
					prsxiixj +=  xi[k+DIMPOINT/2] * yj[k+DIMPOINT/2];
				}

				r2 *= 0.5f * weightGeom;

				r2 +=  weightGrass * (2 - 2 * prsxiixj*prsxiixj / (normxij * normxii));
				/*r2 += weightGrass * (1 - prsxiixj * rsqrt(normxij * normxii) );*/

				for(int k=0; k<DIMVECT; k++)
				{
					TYPE s = exp((-r2 + alphai[k] + betaj[k] )/ epsilon);
					gammai[k] += s;
				}
			}
		}
		__syncthreads();
	}

	// Save the result in global memory. We use a regularized log to prevent floating point errors
	if(i<nx)
	{
		TYPE si = 0.0f;
		for(int k=0; k<DIMVECT; k++)
			si += lambda * (epsilon * log(deltai[k]) - epsilon * log(gammai[k] + EPSILON) + alphai[k]);

		gamma[i] = si;
	}
}


//---------------------------------------------------------------------------------------------------//
//                                     Compute Wdual                                                 //
//---------------------------------------------------------------------------------------------------//


template < typename TYPE, int DIMPOINT, int DIMVECT >
__global__ void WdualOnDevice(TYPE epsilon, TYPE weightGeom, TYPE weightGrass,
        TYPE *alpha, TYPE *x, TYPE *y, TYPE *beta, TYPE *gamma,
        int nx, int ny)
{
	int i = blockIdx.x * blockDim.x + threadIdx.x;

	// the following line does not work with nvcc 3.0 (it is a bug; it works with anterior and posterior versions)
	// extern __shared__ TYPE SharedData[];  // shared data will contain x and alpha data for the block
	// here is the bug fix (see http://forums.nvidia.com/index.php?showtopic=166905)
	extern __shared__ char SharedData_char[];
	TYPE* const SharedData = reinterpret_cast<TYPE*>(SharedData_char);
	// end of bug fix

	int const inc = DIMPOINT + DIMVECT;

	TYPE xi[DIMPOINT], alphai[DIMVECT],  gammai[DIMVECT], normxii = 0.0f;
	if(i<nx)  // we will compute gammai only if i is in the range
	{
		// load xi, alphai, betai from device global memory
		for(int k=0; k<DIMPOINT; k++)
			xi[k] = x[i*DIMPOINT+k];
		for(int k=0; k<DIMVECT; k++)
			alphai[k] = alpha[i*DIMVECT+k];
		for(int k=0; k<DIMVECT; k++)
			gammai[k] = 0.0f;

		for (int k=0; k< DIMPOINT; k++)
			normxii +=  xi[k+DIMPOINT/2] * xi[k+DIMPOINT/2];
	}

	for(int jstart = 0, tile = 0; jstart < ny; jstart += blockDim.x, tile++)
	{
		int j = tile * blockDim.x + threadIdx.x;
		if(j<ny) // we load xj, alphaj and betaj from device global memory only if j<ny
		{
			for(int k=0; k<DIMPOINT; k++)
				SharedData[threadIdx.x*inc+k] = y[j*DIMPOINT+k];
			for(int k=0; k<DIMVECT; k++)
				SharedData[threadIdx.x*inc+DIMPOINT+k] = beta[j*DIMVECT+k];
		}
		__syncthreads();

		if(i<nx) // we compute gammai only if i is in the range
		{

			TYPE *yj = SharedData, *betaj = SharedData + DIMPOINT;
			for(int jrel = 0; jrel < blockDim.x && jrel<ny-jstart; jrel++, yj+=inc, betaj+=inc)
			{
				TYPE r2 = 0.0f,  prsxiixj =0.0f, normxij = 0.0f;
				for(int k=0; k<DIMPOINT/2; k++)
				{
					TYPE ximxj =  xi[k]-yj[k];
					r2 += ximxj * ximxj;

					normxij +=  yj[k+DIMPOINT/2] * yj[k+DIMPOINT/2];
					prsxiixj +=  xi[k+DIMPOINT/2] * yj[k+DIMPOINT/2];
				}

				r2 *= 0.5f * weightGeom;

				r2 +=  weightGrass * (2 - 2 * prsxiixj*prsxiixj / (normxij * normxii));
				/*r2 += weightGrass * (1 - prsxiixj * rsqrt(normxij * normxii) );*/

				for(int k=0; k<DIMVECT; k++)
				{
					TYPE s = exp((-r2 + alphai[k] + betaj[k] )/ epsilon);
					gammai[k] += s;
				}
			}
		}
		__syncthreads();
	}

	// Save the result in global memory.
	if(i<nx)
	{
		TYPE si = 0.0f;
		for(int k=0; k<DIMVECT; k++)
			si +=  -epsilon * gammai[k];

		gamma[i] = si;
	}
}

//---------------------------------------------------------------------------------------------------//
//                                     Compute grad_x                                                //
//---------------------------------------------------------------------------------------------------//


template < typename TYPE, int DIMPOINT, int DIMVECT >
__global__ void GradxWOnDevice(TYPE epsilon,  TYPE weightGeom, TYPE weightGrass,
        TYPE *alpha, TYPE *x, TYPE *y, TYPE *beta, TYPE *gamma,
        int nx, int ny)
{
	int i = blockIdx.x * blockDim.x + threadIdx.x;

	// the following line does not work with nvcc 3.0 (it is a bug; it works with anterior and posterior versions)
	// extern __shared__ TYPE SharedData[];  // shared data will contain x and alpha data for the block
	// here is the bug fix (see http://forums.nvidia.com/index.php?showtopic=166905)
	extern __shared__ char SharedData_char[];
	TYPE* const SharedData = reinterpret_cast<TYPE*>(SharedData_char);
	// end of bug fix

	int const inc = DIMPOINT + DIMVECT;

	TYPE xi[DIMPOINT], alphai[DIMVECT],  gammai[DIMPOINT], normxii = 0.0f;
	if(i<nx)  // we will compute gammai only if i is in the range
	{
		// load xi, alphai, betai from device global memory
		for(int k=0; k<DIMPOINT; k++)
			xi[k] = x[i*DIMPOINT+k];
		for(int k=0; k<DIMVECT; k++)
			alphai[k] = alpha[i*DIMVECT+k];
		for(int k=0; k<DIMPOINT; k++)
			gammai[k] = 0.0f;

		for (int k=0; k< DIMPOINT; k++)
			normxii +=  xi[k+DIMPOINT/2] * xi[k+DIMPOINT/2];
	}

	for(int jstart = 0, tile = 0; jstart < ny; jstart += blockDim.x, tile++)
	{
		int j = tile * blockDim.x + threadIdx.x;
		if(j<ny) // we load xj, alphaj and betaj from device global memory only if j<ny
		{
			for(int k=0; k<DIMPOINT; k++)
				SharedData[threadIdx.x*inc+k] = y[j*DIMPOINT+k];
			for(int k=0; k<DIMVECT; k++)
				SharedData[threadIdx.x*inc+DIMPOINT+k] = beta[j*DIMVECT+k];
		}
		__syncthreads();

		if(i<nx) // we compute gammai only if i is in the range
		{
			TYPE *yj = SharedData, *betaj = SharedData + DIMPOINT;
			for(int jrel = 0; jrel < blockDim.x && jrel<ny-jstart; jrel++, yj+=inc, betaj+=inc)
			{
				TYPE r2 = 0.0f, ximxj[DIMPOINT],  prsxiixj =0.0f, normxij = 0.0f;
				for(int k=0; k<DIMPOINT/2; k++)
				{
					ximxj[k] =  xi[k]-yj[k];
					r2 += ximxj[k]*ximxj[k];

					normxij +=  yj[k+DIMPOINT/2] * yj[k+DIMPOINT/2];
					prsxiixj +=  xi[k+DIMPOINT/2] * yj[k+DIMPOINT/2];
				}

				r2 *= 0.5f * weightGeom;

				r2 +=  weightGrass * (2 - 2 * prsxiixj*prsxiixj / (normxij * normxii));
				/*r2 += weightGrass * (1 - prsxiixj * rsqrt(normxij * normxii) );*/

				TYPE s =  0.0f;
				for(int k=0; k<DIMVECT; k++)
				       	s += exp((-r2 + alphai[k] + betaj[k] )/ epsilon);

				for(int k=0; k<DIMPOINT/2; k++)
				{
					gammai[k] +=weightGeom *  ximxj[k] *s;
					/*gammai[k+DIMPOINT/2] -= weightGrass * ( yj[k+DIMPOINT/2] - prsxiixj * xi[k+DIMPOINT/2] / normxii ) * rsqrt(normxij*normxii) *s;*/
					gammai[k+DIMPOINT/2] -= 8 * prsxiixj * weightGrass * ( yj[k+DIMPOINT/2] - prsxiixj * xi[k+DIMPOINT/2] / normxii ) / (normxij*normxii) *s;
				}
			}
		}
		__syncthreads();
	}

	// Save the result in global memory.
	if(i<nx)
	{
		for(int k=0; k<DIMPOINT; k++)
			gamma[i*DIMPOINT + k] = gammai[k];
	}
}



